{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LoRA Fine-Tuning Notebook\n",
        "\n",
        "This notebook mirrors the functionality of `lora_finetune.py` and allows you to fine-tune a causal language model with LoRA adapters on a JSONL dataset. Update the configuration in the final cell and execute the notebook top to bottom to launch training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ScriptArguments:\n",
        "    \"\"\"Container for argument defaults mirroring the CLI script.\"\"\"\n",
        "\n",
        "    model_name_or_path: str = \"meta-llama/Llama-2-7b-hf\"\n",
        "    train_file: str = \"train.jsonl\"\n",
        "    validation_file: Optional[str] = None\n",
        "    output_dir: str = \"lora-finetuned-model\"\n",
        "    max_seq_length: int = 1024\n",
        "    per_device_train_batch_size: int = 1\n",
        "    per_device_eval_batch_size: int = 1\n",
        "    learning_rate: float = 2e-4\n",
        "    num_train_epochs: float = 3.0\n",
        "    lr_scheduler_type: str = \"cosine\"\n",
        "    warmup_ratio: float = 0.03\n",
        "    weight_decay: float = 0.0\n",
        "    gradient_accumulation_steps: int = 16\n",
        "    logging_steps: int = 10\n",
        "    save_steps: int = 500\n",
        "    save_total_limit: int = 2\n",
        "    lora_r: int = 64\n",
        "    lora_alpha: int = 16\n",
        "    lora_dropout: float = 0.05\n",
        "    template: str = \"### Input\\n{text}\\n\\n### Response\\n{target}\"\n",
        "    input_field: str = \"text\"\n",
        "    target_field: str = \"target\"\n",
        "    gradient_checkpointing: bool = False\n",
        "    use_4bit: bool = False\n",
        "    bnb_dtype: str = \"bfloat16\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_prompt(template: str, text: str, target: str) -> str:\n",
        "    \"\"\"Format a single prompt-response pair according to the provided template.\"\"\"\n",
        "    if \"{text}\" not in template or \"{target}\" not in template:\n",
        "        raise ValueError(\"Template must include '{text}' and '{target}' placeholders.\")\n",
        "    return template.replace(\"{text}\", text).replace(\"{target}\", target)\n",
        "\n",
        "\n",
        "def preprocess_dataset(\n",
        "    tokenizer: AutoTokenizer,\n",
        "    dataset,\n",
        "    template: str,\n",
        "    input_field: str,\n",
        "    target_field: str,\n",
        "    max_seq_length: int,\n",
        ") -> Dict[str, List[int]]:\n",
        "    \"\"\"Apply the prompt template and tokenize the dataset.\"\"\"\n",
        "    def _format_and_tokenize(batch: Dict[str, List[str]]) -> Dict[str, List[List[int]]]:\n",
        "        prompts = [\n",
        "            build_prompt(template, text, target)\n",
        "            for text, target in zip(batch[input_field], batch[target_field])\n",
        "        ]\n",
        "        tokenized = tokenizer(\n",
        "            prompts,\n",
        "            max_length=max_seq_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"np\",\n",
        "        )\n",
        "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "        return tokenized\n",
        "\n",
        "    return dataset.map(_format_and_tokenize, batched=True, remove_columns=dataset.column_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Routine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_training(args: ScriptArguments):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    quantization_config = None\n",
        "    if args.use_4bit:\n",
        "        from transformers import BitsAndBytesConfig\n",
        "\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=getattr(__import__(\"torch\"), args.bnb_dtype),\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        device_map=\"auto\" if quantization_config else None,\n",
        "        quantization_config=quantization_config,\n",
        "    )\n",
        "\n",
        "    if quantization_config is not None:\n",
        "        from peft import prepare_model_for_kbit_training\n",
        "\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=args.lora_r,\n",
        "        lora_alpha=args.lora_alpha,\n",
        "        lora_dropout=args.lora_dropout,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    dataset_dict = {\"train\": args.train_file}\n",
        "    if args.validation_file:\n",
        "        dataset_dict[\"validation\"] = args.validation_file\n",
        "\n",
        "    dataset = load_dataset(\"json\", data_files=dataset_dict)\n",
        "\n",
        "    tokenized_datasets = {\n",
        "        split: preprocess_dataset(\n",
        "            tokenizer,\n",
        "            dataset[split],\n",
        "            args.template,\n",
        "            args.input_field,\n",
        "            args.target_field,\n",
        "            args.max_seq_length,\n",
        "        )\n",
        "        for split in dataset\n",
        "    }\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=args.output_dir,\n",
        "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "        learning_rate=args.learning_rate,\n",
        "        num_train_epochs=args.num_train_epochs,\n",
        "        lr_scheduler_type=args.lr_scheduler_type,\n",
        "        warmup_ratio=args.warmup_ratio,\n",
        "        weight_decay=args.weight_decay,\n",
        "        logging_steps=args.logging_steps,\n",
        "        save_steps=args.save_steps,\n",
        "        save_total_limit=args.save_total_limit,\n",
        "        evaluation_strategy=\"steps\" if \"validation\" in tokenized_datasets else \"no\",\n",
        "        fp16=not args.use_4bit,\n",
        "        bf16=args.use_4bit,\n",
        "        gradient_checkpointing=args.gradient_checkpointing,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets.get(\"validation\"),\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model()\n",
        "    tokenizer.save_pretrained(args.output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "script_args = ScriptArguments(\n",
        "    model_name_or_path=\"meta-llama/Llama-2-7b-hf\",\n",
        "    train_file=\"train.jsonl\",\n",
        "    validation_file=None,\n",
        "    output_dir=\"lora-finetuned-model\",\n",
        ")\n",
        "\n",
        "# Uncomment the line below when you are ready to start training.\n",
        "# run_training(script_args)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}